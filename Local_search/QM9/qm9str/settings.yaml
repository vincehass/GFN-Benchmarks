# run
setting: "qm9str"

# wandb
wandb_mode: "disabled" # 'online', 'disabled'
wandb_project: "local_search"
wandb_entity: "nadhirvincenthassen"
wandb_name: "QM9_tb"

# model
model: gfn # choices=['gfn', 'mars', 'a2c', 'sql', 'ppo']

# local search
ls: true #false, true

# loss_type
loss_type: tb # tb, db, subtb, gtb, maxent

# ls-gfn specific parameters
filtering: deterministic # choices=[deterministic, stochastic]
num_iterations: 7
num_back_forth_steps: 3

# seed for reproduction
seed: 0

# model learning
lr_z: 1.0e-2
lr_policy: 1.0e-4
lr_critic: 1.0e-4
lr_logF: 1.0e-4
clip_policy_logit_min: -20.0
clip_policy_logit_max: 20.0
clip_grad_norm: 10.0
clip_param: 0.2
init_logz: false

# model specific parameters
entropy_coef: 0.01
sql_alpha: 0.01
lamda: 0.9
guide: "uniform" # choices=['substructure', 'uniform']
# CAREFUL - guide_scores_back_policy_traj can have high variance in
# training losses because trajectorie sampled under back policy can have
# extremely low logp under guide
# choices=['guide_scores_back_policy_traj', 'guide_resamples_traj'])
offline_style: "guide_resamples_traj"
parallelize: True
num_guide_workers: 8
guide_sampling_temperature: 1.0

# model architecture
sa_or_ssr: "ssr"
ssr_encoder_hid_dim: 64
ssr_encoder_n_layers: 1
ssr_embed_dim: 64
ssr_scorer_hid_dim: 64
ssr_scorer_n_layers: 1
sa_hid_dim: 64
sa_n_layers: 2

# trainer
num_active_learning_rounds: 200
num_samples_per_online_batch: 32
num_samples_per_offline_batch: 32

num_steps_per_batch: 1
num_online_batches_per_round: 1
num_offline_batches_per_round: 1
target_mix_backpolicy_weight: 0.5

prt: true
explore_epsilon: 0.10

# logging
saved_models_dir: "saved_models/qm9str/"
save_every_x_active_rounds: 50

monitor_num_samples: 128
monitor_fast_every: 10
monitor_slow_every: 200
monitor_real_samples: true

# reward exponent and normalization constant
scale_reward_min: 0.001
scale_reward_max: 100
reward_exp: 5

# experiment-specific settings
blocks_file: datasets/qm9str/block_qm9str_v1.json
x_to_r_file: datasets/qm9str/block_qm9str_v1_s5.pkl
mode_file: datasets/qm9str/modes_qm9str.pkl
forced_stop_len: 5
